{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOhlPaI3puzvQ5BLwI8sOk7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nofryntii/202055202008-Nofryanti/blob/master/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapipe"
      ],
      "metadata": {
        "id": "_GQbaiSDNXGA",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install mediapipe-model-maker"
      ],
      "metadata": {
        "id": "ZkdpuopONkKn",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pathlib\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "from mediapipe_model_maker.python.vision import gesture_recognizer\n",
        "from mediapipe.tasks.python.vision.gesture_recognizer import GestureRecognizer\n",
        "from mediapipe.framework.formats import landmark_pb2\n"
      ],
      "metadata": {
        "id": "4ngwxmXFMtw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "TbTfCL_rOLXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_root = pathlib.Path(\"/content/drive/MyDrive/Sibi\")"
      ],
      "metadata": {
        "id": "2UNyB0qcOT_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import utils"
      ],
      "metadata": {
        "id": "MJ8GP_mKOhuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menampilkan label dari dataset\n",
        "labels = [p.name for p in dataset_root.iterdir() if p.is_dir()]\n",
        "print(f\"Classes: {labels}\")\n",
        "\n",
        "# Mendapatkan daftar file gambar\n",
        "train_files = utils.find_images(dataset_root)\n",
        "print(f\"Number of training images: {len(train_files)}\")\n"
      ],
      "metadata": {
        "id": "lQmAxyg3O1R_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_and_convert_images(dataset_path):\n",
        "    for root, dirs, files in os.walk(dataset_path):\n",
        "        for file in files:\n",
        "            if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                img_path = os.path.join(root, file)\n",
        "                img = cv2.imread(img_path)\n",
        "                if img is not None:\n",
        "                    # Konversi ke RGB\n",
        "                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "                    # Resize gambar ke ukuran yang diinginkan\n",
        "                    img = cv2.resize(img, (192, 192))\n",
        "                    # Simpan kembali\n",
        "                    cv2.imwrite(img_path, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "# Jalankan fungsi konversi\n",
        "check_and_convert_images(dataset_root)"
      ],
      "metadata": {
        "id": "nLu6nN_RytJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "\n",
        "def check_and_convert_images(dataset_path):\n",
        "    for root, dirs, files in os.walk(dataset_path):\n",
        "        for file in files:\n",
        "            if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                img_path = os.path.join(root, file)\n",
        "\n",
        "                # Membaca gambar\n",
        "                img = cv2.imread(img_path)\n",
        "\n",
        "                if img is not None:\n",
        "                    # Cek jumlah channel gambar\n",
        "                    if len(img.shape) == 2:  # Jika gambar grayscale (1 channel)\n",
        "                        print(f\"Gambar grayscale terdeteksi: {img_path}\")\n",
        "                        # Konversi gambar grayscale menjadi RGB\n",
        "                        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
        "                    elif img.shape[2] == 3:  # Jika gambar berwarna (3 channel)\n",
        "                        # Konversi dari BGR ke RGB\n",
        "                        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "                    # Resize gambar ke ukuran yang diinginkan\n",
        "                    img = cv2.resize(img, (192, 192))\n",
        "\n",
        "                    # Simpan kembali gambar dengan format BGR\n",
        "                    cv2.imwrite(img_path, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "# Jalankan fungsi konversi\n",
        "dataset_root = \"/content/drive/MyDrive/Sibi\"\n",
        "check_and_convert_images(dataset_root)\n"
      ],
      "metadata": {
        "id": "y6fKVO6hqh9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def verify_dataset_structure(dataset_path):\n",
        "    # Pastikan folder dataset ada\n",
        "    if not os.path.exists(dataset_path):\n",
        "        raise ValueError(f\"Dataset path {dataset_path} tidak ditemukan\")\n",
        "\n",
        "    # Variabel untuk menghitung total gambar\n",
        "    total_images = 0\n",
        "\n",
        "    # Hitung jumlah gambar per kelas\n",
        "    for class_name in os.listdir(dataset_path):\n",
        "        class_path = os.path.join(dataset_path, class_name)\n",
        "        if os.path.isdir(class_path):\n",
        "            n_images = len([f for f in os.listdir(class_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
        "            print(f\"Kelas {class_name}: {n_images} gambar\")\n",
        "            # Tambahkan jumlah gambar ke total\n",
        "            total_images += n_images\n",
        "\n",
        "    # Tampilkan total keseluruhan gambar\n",
        "    print(f\"\\nTotal keseluruhan gambar: {total_images} gambar\")\n",
        "\n",
        "# Menjalankan fungsi untuk memeriksa dataset\n",
        "verify_dataset_structure(dataset_root)\n"
      ],
      "metadata": {
        "id": "p5riBeKUt29u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "import shutil\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def split_dataset(input_root, output_root, splits=\"80:20\", seed=None):\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "\n",
        "    input_root = pathlib.Path(input_root)\n",
        "    output_root = pathlib.Path(output_root)\n",
        "\n",
        "    # Membagi persentase menjadi angka\n",
        "    split_percentages = [int(s) for s in splits.split(':')]\n",
        "\n",
        "    # Memastikan persentase berjumlah 100\n",
        "    assert sum(split_percentages) == 100, \"Total persentase harus 100\"\n",
        "\n",
        "    for labelpath in input_root.iterdir():\n",
        "        if not labelpath.is_dir():\n",
        "            continue\n",
        "\n",
        "        files = sorted(labelpath.iterdir())\n",
        "        np.random.shuffle(files)\n",
        "\n",
        "        # Hitung jumlah gambar untuk setiap split\n",
        "        total_files = len(files)\n",
        "        train_size = int(total_files * (split_percentages[0] / 100))\n",
        "        test_size = total_files - train_size\n",
        "\n",
        "        # Membagi dataset\n",
        "        subsets = {\n",
        "            'train': files[:train_size],\n",
        "            'test': files[train_size:]\n",
        "        }\n",
        "\n",
        "        # Menyimpan dataset ke folder output\n",
        "        for split_name, subset_files in subsets.items():\n",
        "            subset_root = output_root / split_name / labelpath.name\n",
        "            subset_root.mkdir(parents=True, exist_ok=True)\n",
        "            for file in subset_files:\n",
        "                shutil.copy(file, subset_root)\n",
        "\n",
        "        print(f\"Kelas {labelpath.name} - Train: {train_size}, Test: {test_size}\")\n",
        "\n",
        "# Menjalankan fungsi split dengan 80% untuk training dan 20% untuk testing\n",
        "output_root = \"/content/processed_data\"\n",
        "split_dataset(dataset_root, output_root, splits=\"80:20\", seed=42)\n"
      ],
      "metadata": {
        "id": "KLLqsG2XO6AY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataset(input_root, output_root, splits=\"80:20\", seed=None):\n",
        "    np.random.seed(seed)\n",
        "    input_root, output_root = pathlib.Path(input_root), pathlib.Path(output_root)\n",
        "    split_percentages = [int(s) for s in splits.split(':')]\n",
        "    assert sum(split_percentages) == 100, \"Total persentase harus 100\"\n",
        "\n",
        "    for labelpath in input_root.iterdir():\n",
        "        if labelpath.is_dir():\n",
        "            files = sorted(labelpath.iterdir())\n",
        "            np.random.shuffle(files)\n",
        "            train_size = int(len(files) * (split_percentages[0] / 100))\n",
        "            subsets = {'train': files[:train_size], 'test': files[train_size:]}\n",
        "\n",
        "            for split_name, subset_files in subsets.items():\n",
        "                (output_root / split_name / labelpath.name).mkdir(parents=True, exist_ok=True)\n",
        "                for file in subset_files:\n",
        "                    shutil.copy(file, output_root / split_name / labelpath.name)\n",
        "\n",
        "            print(f\"Kelas {labelpath.name} - Train: {train_size}, Test: {len(files) - train_size}\")\n",
        "\n",
        "# Usage\n",
        "split_dataset(dataset_root, \"/content/processed_data\", splits=\"80:20\", seed=42)\n"
      ],
      "metadata": {
        "id": "Oqj2HlBaVOdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = gesture_recognizer.Dataset.from_folder(str(dataset_root))\n",
        "train_data.gen_tf_dataset().unbatch().save(\"/content/train_data\")"
      ],
      "metadata": {
        "id": "pxMsTp7-RecP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_root = pathlib.Path(\"/content/processed_data\")"
      ],
      "metadata": {
        "id": "DJQ5WNiCZO7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "import numpy as np\n",
        "\n",
        "import utils\n",
        "\n",
        "data_root = pathlib.Path(\"./processed_data\")\n",
        "dataset_train = data_root / \"train\"\n",
        "trainfiles = utils.find_images(dataset_train)\n",
        "\n",
        "sample_files = np.random.choice(np.asarray(trainfiles), 10)\n",
        "fig, axarr = utils.plot_image_files(sample_files, ncols=5)\n",
        "fig.savefig(\"example-output.jpg\", dpi=150, bbox_inches=\"tight\")"
      ],
      "metadata": {
        "id": "aDwaujag63qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VFHzSQFh62tj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mediapipe_model_maker.python.vision import gesture_recognizer\n",
        "\n",
        "handparams = gesture_recognizer.HandDataPreprocessingParams(\n",
        "    min_detection_confidence=0.5\n",
        ")\n",
        "\n",
        "dataset_train = data_root / \"train\"\n",
        "data = gesture_recognizer.Dataset.from_folder(str(dataset_train), handparams)\n",
        "train_data, validation_data = data.split(0.8)\n",
        "\n",
        "dataset_test = data_root / \"test\"\n",
        "test_data = gesture_recognizer.Dataset.from_folder(\n",
        "    str(dataset_test), handparams\n",
        ")\n"
      ],
      "metadata": {
        "id": "XeTn3-RoUo_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainfiles = utils.find_images(dataset_train)\n",
        "\n",
        "sample_files = np.random.choice(np.asarray(trainfiles), 10)\n",
        "fig, axarr = utils.plot_image_files(sample_files, ncols=5)\n",
        "fig.savefig(\"outputdataset.jpg\", dpi=150, bbox_inches=\"tight\")"
      ],
      "metadata": {
        "id": "qTqpjm80WrNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hparams = gesture_recognizer.HParams(\n",
        "    export_dir=\"exported_model\",\n",
        "    batch_size=32,\n",
        "    epochs=100,\n",
        "    shuffle=True,\n",
        "    learning_rate=0.001,\n",
        "    lr_decay=0.95,\n",
        ")\n",
        "moptions = gesture_recognizer.ModelOptions(dropout_rate=0.05)\n",
        "options = gesture_recognizer.GestureRecognizerOptions(\n",
        "    hparams=hparams, model_options=moptions\n",
        ")\n",
        "\n",
        "model = gesture_recognizer.GestureRecognizer.create(\n",
        "    train_data=train_data, validation_data=validation_data, options=options\n",
        ")\n"
      ],
      "metadata": {
        "id": "2bS8AvXcWdMb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23e61aa9-196d-4bfa-a7d3-f292ff1bb626"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " hand_embedding (InputLayer  [(None, 128)]             0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 128)               512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " re_lu_2 (ReLU)              (None, 128)               0         \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " custom_gesture_recognizer_  (None, 25)                3225      \n",
            " out (Dense)                                                     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3737 (14.60 KB)\n",
            "Trainable params: 3481 (13.60 KB)\n",
            "Non-trainable params: 256 (1.00 KB)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Resuming from exported_model/epoch_models/model-0100\n",
            "Epoch 1/100\n",
            "124/124 [==============================] - 9s 63ms/step - loss: 0.1453 - categorical_accuracy: 0.9186 - val_loss: 0.1534 - val_categorical_accuracy: 0.9268 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "124/124 [==============================] - 5s 36ms/step - loss: 0.1381 - categorical_accuracy: 0.9176 - val_loss: 0.1513 - val_categorical_accuracy: 0.9288 - lr: 9.5000e-04\n",
            "Epoch 3/100\n",
            "124/124 [==============================] - 7s 52ms/step - loss: 0.1382 - categorical_accuracy: 0.9166 - val_loss: 0.1521 - val_categorical_accuracy: 0.9288 - lr: 9.0250e-04\n",
            "Epoch 4/100\n",
            "124/124 [==============================] - 5s 38ms/step - loss: 0.1380 - categorical_accuracy: 0.9141 - val_loss: 0.1549 - val_categorical_accuracy: 0.9278 - lr: 8.5737e-04\n",
            "Epoch 5/100\n",
            "124/124 [==============================] - 7s 54ms/step - loss: 0.1369 - categorical_accuracy: 0.9234 - val_loss: 0.1541 - val_categorical_accuracy: 0.9248 - lr: 8.1451e-04\n",
            "Epoch 6/100\n",
            "124/124 [==============================] - 5s 36ms/step - loss: 0.1356 - categorical_accuracy: 0.9158 - val_loss: 0.1516 - val_categorical_accuracy: 0.9298 - lr: 7.7378e-04\n",
            "Epoch 7/100\n",
            "124/124 [==============================] - 7s 53ms/step - loss: 0.1362 - categorical_accuracy: 0.9183 - val_loss: 0.1502 - val_categorical_accuracy: 0.9278 - lr: 7.3509e-04\n",
            "Epoch 8/100\n",
            "124/124 [==============================] - 5s 36ms/step - loss: 0.1287 - categorical_accuracy: 0.9259 - val_loss: 0.1526 - val_categorical_accuracy: 0.9268 - lr: 6.9834e-04\n",
            "Epoch 9/100\n",
            "124/124 [==============================] - 8s 61ms/step - loss: 0.1401 - categorical_accuracy: 0.9181 - val_loss: 0.1512 - val_categorical_accuracy: 0.9268 - lr: 6.6342e-04\n",
            "Epoch 10/100\n",
            "124/124 [==============================] - 5s 36ms/step - loss: 0.1342 - categorical_accuracy: 0.9201 - val_loss: 0.1504 - val_categorical_accuracy: 0.9308 - lr: 6.3025e-04\n",
            "Epoch 11/100\n",
            "124/124 [==============================] - 8s 61ms/step - loss: 0.1301 - categorical_accuracy: 0.9226 - val_loss: 0.1498 - val_categorical_accuracy: 0.9298 - lr: 5.9874e-04\n",
            "Epoch 12/100\n",
            "124/124 [==============================] - 5s 38ms/step - loss: 0.1301 - categorical_accuracy: 0.9224 - val_loss: 0.1486 - val_categorical_accuracy: 0.9308 - lr: 5.6880e-04\n",
            "Epoch 13/100\n",
            "124/124 [==============================] - 7s 54ms/step - loss: 0.1299 - categorical_accuracy: 0.9262 - val_loss: 0.1513 - val_categorical_accuracy: 0.9258 - lr: 5.4036e-04\n",
            "Epoch 14/100\n",
            "124/124 [==============================] - 5s 37ms/step - loss: 0.1252 - categorical_accuracy: 0.9211 - val_loss: 0.1491 - val_categorical_accuracy: 0.9308 - lr: 5.1334e-04\n",
            "Epoch 15/100\n",
            "124/124 [==============================] - 7s 54ms/step - loss: 0.1292 - categorical_accuracy: 0.9246 - val_loss: 0.1499 - val_categorical_accuracy: 0.9308 - lr: 4.8767e-04\n",
            "Epoch 16/100\n",
            "124/124 [==============================] - 5s 37ms/step - loss: 0.1269 - categorical_accuracy: 0.9226 - val_loss: 0.1515 - val_categorical_accuracy: 0.9288 - lr: 4.6329e-04\n",
            "Epoch 17/100\n",
            "124/124 [==============================] - 8s 61ms/step - loss: 0.1262 - categorical_accuracy: 0.9194 - val_loss: 0.1493 - val_categorical_accuracy: 0.9308 - lr: 4.4013e-04\n",
            "Epoch 18/100\n",
            "124/124 [==============================] - 5s 37ms/step - loss: 0.1304 - categorical_accuracy: 0.9257 - val_loss: 0.1484 - val_categorical_accuracy: 0.9298 - lr: 4.1812e-04\n",
            "Epoch 19/100\n",
            "124/124 [==============================] - 7s 54ms/step - loss: 0.1328 - categorical_accuracy: 0.9209 - val_loss: 0.1473 - val_categorical_accuracy: 0.9298 - lr: 3.9721e-04\n",
            "Epoch 20/100\n",
            "124/124 [==============================] - 5s 36ms/step - loss: 0.1340 - categorical_accuracy: 0.9178 - val_loss: 0.1486 - val_categorical_accuracy: 0.9288 - lr: 3.7735e-04\n",
            "Epoch 21/100\n",
            "124/124 [==============================] - 5s 37ms/step - loss: 0.1283 - categorical_accuracy: 0.9246 - val_loss: 0.1483 - val_categorical_accuracy: 0.9298 - lr: 3.5849e-04\n",
            "Epoch 22/100\n",
            "124/124 [==============================] - 7s 53ms/step - loss: 0.1266 - categorical_accuracy: 0.9239 - val_loss: 0.1477 - val_categorical_accuracy: 0.9278 - lr: 3.4056e-04\n",
            "Epoch 23/100\n",
            "124/124 [==============================] - 5s 37ms/step - loss: 0.1278 - categorical_accuracy: 0.9236 - val_loss: 0.1484 - val_categorical_accuracy: 0.9298 - lr: 3.2353e-04\n",
            "Epoch 24/100\n",
            "124/124 [==============================] - 7s 52ms/step - loss: 0.1189 - categorical_accuracy: 0.9284 - val_loss: 0.1477 - val_categorical_accuracy: 0.9298 - lr: 3.0736e-04\n",
            "Epoch 25/100\n",
            "124/124 [==============================] - 5s 40ms/step - loss: 0.1259 - categorical_accuracy: 0.9244 - val_loss: 0.1489 - val_categorical_accuracy: 0.9288 - lr: 2.9199e-04\n",
            "Epoch 26/100\n",
            "124/124 [==============================] - 7s 50ms/step - loss: 0.1261 - categorical_accuracy: 0.9254 - val_loss: 0.1497 - val_categorical_accuracy: 0.9298 - lr: 2.7739e-04\n",
            "Epoch 27/100\n",
            "124/124 [==============================] - 6s 44ms/step - loss: 0.1252 - categorical_accuracy: 0.9259 - val_loss: 0.1482 - val_categorical_accuracy: 0.9268 - lr: 2.6352e-04\n",
            "Epoch 28/100\n",
            "124/124 [==============================] - 5s 36ms/step - loss: 0.1206 - categorical_accuracy: 0.9292 - val_loss: 0.1475 - val_categorical_accuracy: 0.9308 - lr: 2.5034e-04\n",
            "Epoch 29/100\n",
            "124/124 [==============================] - 7s 53ms/step - loss: 0.1210 - categorical_accuracy: 0.9229 - val_loss: 0.1481 - val_categorical_accuracy: 0.9308 - lr: 2.3783e-04\n",
            "Epoch 30/100\n",
            "124/124 [==============================] - 5s 36ms/step - loss: 0.1203 - categorical_accuracy: 0.9272 - val_loss: 0.1481 - val_categorical_accuracy: 0.9278 - lr: 2.2594e-04\n",
            "Epoch 31/100\n",
            "124/124 [==============================] - 7s 54ms/step - loss: 0.1237 - categorical_accuracy: 0.9246 - val_loss: 0.1486 - val_categorical_accuracy: 0.9278 - lr: 2.1464e-04\n",
            "Epoch 32/100\n",
            "124/124 [==============================] - 5s 37ms/step - loss: 0.1265 - categorical_accuracy: 0.9269 - val_loss: 0.1484 - val_categorical_accuracy: 0.9298 - lr: 2.0391e-04\n",
            "Epoch 33/100\n",
            "124/124 [==============================] - 5s 40ms/step - loss: 0.1178 - categorical_accuracy: 0.9267 - val_loss: 0.1481 - val_categorical_accuracy: 0.9288 - lr: 1.9371e-04\n",
            "Epoch 34/100\n",
            "124/124 [==============================] - 6s 49ms/step - loss: 0.1232 - categorical_accuracy: 0.9262 - val_loss: 0.1479 - val_categorical_accuracy: 0.9288 - lr: 1.8403e-04\n",
            "Epoch 35/100\n",
            "124/124 [==============================] - 8s 62ms/step - loss: 0.1207 - categorical_accuracy: 0.9262 - val_loss: 0.1481 - val_categorical_accuracy: 0.9258 - lr: 1.7482e-04\n",
            "Epoch 36/100\n",
            "124/124 [==============================] - 5s 37ms/step - loss: 0.1223 - categorical_accuracy: 0.9282 - val_loss: 0.1473 - val_categorical_accuracy: 0.9298 - lr: 1.6608e-04\n",
            "Epoch 37/100\n",
            "124/124 [==============================] - 8s 61ms/step - loss: 0.1166 - categorical_accuracy: 0.9269 - val_loss: 0.1476 - val_categorical_accuracy: 0.9278 - lr: 1.5778e-04\n",
            "Epoch 38/100\n",
            "124/124 [==============================] - 5s 37ms/step - loss: 0.1161 - categorical_accuracy: 0.9259 - val_loss: 0.1481 - val_categorical_accuracy: 0.9278 - lr: 1.4989e-04\n",
            "Epoch 39/100\n",
            "124/124 [==============================] - 7s 55ms/step - loss: 0.1226 - categorical_accuracy: 0.9244 - val_loss: 0.1477 - val_categorical_accuracy: 0.9268 - lr: 1.4240e-04\n",
            "Epoch 40/100\n",
            "124/124 [==============================] - 5s 38ms/step - loss: 0.1177 - categorical_accuracy: 0.9302 - val_loss: 0.1476 - val_categorical_accuracy: 0.9278 - lr: 1.3528e-04\n",
            "Epoch 41/100\n",
            "124/124 [==============================] - 5s 39ms/step - loss: 0.1166 - categorical_accuracy: 0.9282 - val_loss: 0.1482 - val_categorical_accuracy: 0.9288 - lr: 1.2851e-04\n",
            "Epoch 42/100\n",
            "124/124 [==============================] - 7s 50ms/step - loss: 0.1192 - categorical_accuracy: 0.9279 - val_loss: 0.1475 - val_categorical_accuracy: 0.9278 - lr: 1.2209e-04\n",
            "Epoch 43/100\n",
            "124/124 [==============================] - 8s 62ms/step - loss: 0.1254 - categorical_accuracy: 0.9234 - val_loss: 0.1471 - val_categorical_accuracy: 0.9288 - lr: 1.1598e-04\n",
            "Epoch 44/100\n",
            "124/124 [==============================] - 5s 36ms/step - loss: 0.1238 - categorical_accuracy: 0.9282 - val_loss: 0.1468 - val_categorical_accuracy: 0.9288 - lr: 1.1018e-04\n",
            "Epoch 45/100\n",
            "124/124 [==============================] - 5s 36ms/step - loss: 0.1248 - categorical_accuracy: 0.9282 - val_loss: 0.1476 - val_categorical_accuracy: 0.9288 - lr: 1.0467e-04\n",
            "Epoch 46/100\n",
            "124/124 [==============================] - 6s 50ms/step - loss: 0.1178 - categorical_accuracy: 0.9257 - val_loss: 0.1477 - val_categorical_accuracy: 0.9288 - lr: 9.9440e-05\n",
            "Epoch 47/100\n",
            "124/124 [==============================] - 8s 61ms/step - loss: 0.1256 - categorical_accuracy: 0.9236 - val_loss: 0.1479 - val_categorical_accuracy: 0.9288 - lr: 9.4468e-05\n",
            "Epoch 48/100\n",
            "124/124 [==============================] - 5s 37ms/step - loss: 0.1190 - categorical_accuracy: 0.9272 - val_loss: 0.1479 - val_categorical_accuracy: 0.9298 - lr: 8.9745e-05\n",
            "Epoch 49/100\n",
            "124/124 [==============================] - 7s 54ms/step - loss: 0.1182 - categorical_accuracy: 0.9284 - val_loss: 0.1476 - val_categorical_accuracy: 0.9308 - lr: 8.5258e-05\n",
            "Epoch 50/100\n",
            "124/124 [==============================] - 5s 40ms/step - loss: 0.1095 - categorical_accuracy: 0.9317 - val_loss: 0.1477 - val_categorical_accuracy: 0.9298 - lr: 8.0995e-05\n",
            "Epoch 51/100\n",
            "124/124 [==============================] - 5s 40ms/step - loss: 0.1175 - categorical_accuracy: 0.9267 - val_loss: 0.1475 - val_categorical_accuracy: 0.9288 - lr: 7.6945e-05\n",
            "Epoch 52/100\n",
            "124/124 [==============================] - 6s 49ms/step - loss: 0.1189 - categorical_accuracy: 0.9267 - val_loss: 0.1474 - val_categorical_accuracy: 0.9298 - lr: 7.3098e-05\n",
            "Epoch 53/100\n",
            "124/124 [==============================] - 6s 44ms/step - loss: 0.1145 - categorical_accuracy: 0.9299 - val_loss: 0.1476 - val_categorical_accuracy: 0.9298 - lr: 6.9443e-05\n",
            "Epoch 54/100\n",
            "124/124 [==============================] - 6s 46ms/step - loss: 0.1142 - categorical_accuracy: 0.9297 - val_loss: 0.1474 - val_categorical_accuracy: 0.9278 - lr: 6.5971e-05\n",
            "Epoch 55/100\n",
            "124/124 [==============================] - 8s 61ms/step - loss: 0.1204 - categorical_accuracy: 0.9315 - val_loss: 0.1478 - val_categorical_accuracy: 0.9298 - lr: 6.2672e-05\n",
            "Epoch 56/100\n",
            "124/124 [==============================] - 5s 37ms/step - loss: 0.1196 - categorical_accuracy: 0.9239 - val_loss: 0.1478 - val_categorical_accuracy: 0.9298 - lr: 5.9539e-05\n",
            "Epoch 57/100\n",
            "124/124 [==============================] - 7s 54ms/step - loss: 0.1138 - categorical_accuracy: 0.9287 - val_loss: 0.1482 - val_categorical_accuracy: 0.9298 - lr: 5.6562e-05\n",
            "Epoch 58/100\n",
            "124/124 [==============================] - 5s 36ms/step - loss: 0.1231 - categorical_accuracy: 0.9236 - val_loss: 0.1480 - val_categorical_accuracy: 0.9288 - lr: 5.3734e-05\n",
            "Epoch 59/100\n",
            "124/124 [==============================] - 5s 38ms/step - loss: 0.1156 - categorical_accuracy: 0.9322 - val_loss: 0.1480 - val_categorical_accuracy: 0.9268 - lr: 5.1047e-05\n",
            "Epoch 60/100\n",
            "124/124 [==============================] - 7s 52ms/step - loss: 0.1204 - categorical_accuracy: 0.9304 - val_loss: 0.1479 - val_categorical_accuracy: 0.9278 - lr: 4.8495e-05\n",
            "Epoch 61/100\n",
            "124/124 [==============================] - 5s 36ms/step - loss: 0.1173 - categorical_accuracy: 0.9234 - val_loss: 0.1474 - val_categorical_accuracy: 0.9268 - lr: 4.6070e-05\n",
            "Epoch 62/100\n",
            "124/124 [==============================] - 6s 50ms/step - loss: 0.1150 - categorical_accuracy: 0.9287 - val_loss: 0.1473 - val_categorical_accuracy: 0.9278 - lr: 4.3766e-05\n",
            "Epoch 63/100\n",
            "124/124 [==============================] - 5s 40ms/step - loss: 0.1154 - categorical_accuracy: 0.9279 - val_loss: 0.1472 - val_categorical_accuracy: 0.9268 - lr: 4.1578e-05\n",
            "Epoch 64/100\n",
            "124/124 [==============================] - 5s 38ms/step - loss: 0.1174 - categorical_accuracy: 0.9272 - val_loss: 0.1471 - val_categorical_accuracy: 0.9288 - lr: 3.9499e-05\n",
            "Epoch 65/100\n",
            "124/124 [==============================] - 7s 54ms/step - loss: 0.1184 - categorical_accuracy: 0.9297 - val_loss: 0.1472 - val_categorical_accuracy: 0.9288 - lr: 3.7524e-05\n",
            "Epoch 66/100\n",
            "124/124 [==============================] - 5s 36ms/step - loss: 0.1154 - categorical_accuracy: 0.9340 - val_loss: 0.1471 - val_categorical_accuracy: 0.9288 - lr: 3.5648e-05\n",
            "Epoch 67/100\n",
            "124/124 [==============================] - 8s 62ms/step - loss: 0.1197 - categorical_accuracy: 0.9292 - val_loss: 0.1471 - val_categorical_accuracy: 0.9288 - lr: 3.3866e-05\n",
            "Epoch 68/100\n",
            "124/124 [==============================] - 5s 37ms/step - loss: 0.1134 - categorical_accuracy: 0.9294 - val_loss: 0.1472 - val_categorical_accuracy: 0.9268 - lr: 3.2172e-05\n",
            "Epoch 69/100\n",
            "124/124 [==============================] - 7s 53ms/step - loss: 0.1187 - categorical_accuracy: 0.9299 - val_loss: 0.1472 - val_categorical_accuracy: 0.9278 - lr: 3.0564e-05\n",
            "Epoch 70/100\n",
            "124/124 [==============================] - 5s 36ms/step - loss: 0.1168 - categorical_accuracy: 0.9257 - val_loss: 0.1471 - val_categorical_accuracy: 0.9288 - lr: 2.9035e-05\n",
            "Epoch 71/100\n",
            "124/124 [==============================] - 5s 35ms/step - loss: 0.1161 - categorical_accuracy: 0.9277 - val_loss: 0.1470 - val_categorical_accuracy: 0.9288 - lr: 2.7584e-05\n",
            "Epoch 72/100\n",
            "124/124 [==============================] - 7s 53ms/step - loss: 0.1180 - categorical_accuracy: 0.9309 - val_loss: 0.1469 - val_categorical_accuracy: 0.9288 - lr: 2.6205e-05\n",
            "Epoch 73/100\n",
            "124/124 [==============================] - 5s 40ms/step - loss: 0.1147 - categorical_accuracy: 0.9277 - val_loss: 0.1469 - val_categorical_accuracy: 0.9288 - lr: 2.4894e-05\n",
            "Epoch 74/100\n",
            "124/124 [==============================] - 6s 50ms/step - loss: 0.1111 - categorical_accuracy: 0.9332 - val_loss: 0.1470 - val_categorical_accuracy: 0.9298 - lr: 2.3650e-05\n",
            "Epoch 75/100\n",
            "124/124 [==============================] - 8s 61ms/step - loss: 0.1207 - categorical_accuracy: 0.9307 - val_loss: 0.1471 - val_categorical_accuracy: 0.9288 - lr: 2.2467e-05\n",
            "Epoch 76/100\n",
            "124/124 [==============================] - 5s 37ms/step - loss: 0.1173 - categorical_accuracy: 0.9287 - val_loss: 0.1471 - val_categorical_accuracy: 0.9288 - lr: 2.1344e-05\n",
            "Epoch 77/100\n",
            "124/124 [==============================] - 7s 54ms/step - loss: 0.1133 - categorical_accuracy: 0.9272 - val_loss: 0.1474 - val_categorical_accuracy: 0.9278 - lr: 2.0277e-05\n",
            "Epoch 78/100\n",
            "124/124 [==============================] - 5s 36ms/step - loss: 0.1185 - categorical_accuracy: 0.9294 - val_loss: 0.1473 - val_categorical_accuracy: 0.9298 - lr: 1.9263e-05\n",
            "Epoch 79/100\n",
            "124/124 [==============================] - 7s 54ms/step - loss: 0.1180 - categorical_accuracy: 0.9284 - val_loss: 0.1474 - val_categorical_accuracy: 0.9288 - lr: 1.8300e-05\n",
            "Epoch 80/100\n",
            "124/124 [==============================] - 5s 36ms/step - loss: 0.1132 - categorical_accuracy: 0.9309 - val_loss: 0.1474 - val_categorical_accuracy: 0.9298 - lr: 1.7385e-05\n",
            "Epoch 81/100\n",
            "124/124 [==============================] - 8s 61ms/step - loss: 0.1135 - categorical_accuracy: 0.9315 - val_loss: 0.1474 - val_categorical_accuracy: 0.9298 - lr: 1.6515e-05\n",
            "Epoch 82/100\n",
            "124/124 [==============================] - 5s 38ms/step - loss: 0.1158 - categorical_accuracy: 0.9274 - val_loss: 0.1472 - val_categorical_accuracy: 0.9298 - lr: 1.5690e-05\n",
            "Epoch 83/100\n",
            "124/124 [==============================] - 5s 39ms/step - loss: 0.1187 - categorical_accuracy: 0.9277 - val_loss: 0.1472 - val_categorical_accuracy: 0.9298 - lr: 1.4905e-05\n",
            "Epoch 84/100\n",
            "124/124 [==============================] - 6s 49ms/step - loss: 0.1180 - categorical_accuracy: 0.9282 - val_loss: 0.1473 - val_categorical_accuracy: 0.9298 - lr: 1.4160e-05\n",
            "Epoch 85/100\n",
            "124/124 [==============================] - 6s 45ms/step - loss: 0.1149 - categorical_accuracy: 0.9287 - val_loss: 0.1472 - val_categorical_accuracy: 0.9298 - lr: 1.3452e-05\n",
            "Epoch 86/100\n",
            "124/124 [==============================] - 6s 44ms/step - loss: 0.1106 - categorical_accuracy: 0.9332 - val_loss: 0.1474 - val_categorical_accuracy: 0.9298 - lr: 1.2779e-05\n",
            "Epoch 87/100\n",
            "124/124 [==============================] - 5s 37ms/step - loss: 0.1142 - categorical_accuracy: 0.9297 - val_loss: 0.1473 - val_categorical_accuracy: 0.9298 - lr: 1.2140e-05\n",
            "Epoch 88/100\n",
            "124/124 [==============================] - 7s 53ms/step - loss: 0.1132 - categorical_accuracy: 0.9252 - val_loss: 0.1473 - val_categorical_accuracy: 0.9298 - lr: 1.1533e-05\n",
            "Epoch 89/100\n",
            "124/124 [==============================] - 5s 36ms/step - loss: 0.1203 - categorical_accuracy: 0.9304 - val_loss: 0.1473 - val_categorical_accuracy: 0.9298 - lr: 1.0957e-05\n",
            "Epoch 90/100\n",
            "124/124 [==============================] - 5s 37ms/step - loss: 0.1136 - categorical_accuracy: 0.9279 - val_loss: 0.1473 - val_categorical_accuracy: 0.9298 - lr: 1.0409e-05\n",
            "Epoch 91/100\n",
            "124/124 [==============================] - 7s 51ms/step - loss: 0.1198 - categorical_accuracy: 0.9284 - val_loss: 0.1473 - val_categorical_accuracy: 0.9298 - lr: 9.8884e-06\n",
            "Epoch 92/100\n",
            "124/124 [==============================] - 5s 40ms/step - loss: 0.1196 - categorical_accuracy: 0.9267 - val_loss: 0.1473 - val_categorical_accuracy: 0.9298 - lr: 9.3939e-06\n",
            "Epoch 93/100\n",
            "124/124 [==============================] - 6s 48ms/step - loss: 0.1134 - categorical_accuracy: 0.9297 - val_loss: 0.1474 - val_categorical_accuracy: 0.9298 - lr: 8.9242e-06\n",
            "Epoch 94/100\n",
            "124/124 [==============================] - 8s 61ms/step - loss: 0.1175 - categorical_accuracy: 0.9279 - val_loss: 0.1473 - val_categorical_accuracy: 0.9298 - lr: 8.4780e-06\n",
            "Epoch 95/100\n",
            "124/124 [==============================] - 5s 37ms/step - loss: 0.1177 - categorical_accuracy: 0.9315 - val_loss: 0.1474 - val_categorical_accuracy: 0.9298 - lr: 8.0541e-06\n",
            "Epoch 96/100\n",
            "124/124 [==============================] - 7s 53ms/step - loss: 0.1134 - categorical_accuracy: 0.9252 - val_loss: 0.1474 - val_categorical_accuracy: 0.9298 - lr: 7.6514e-06\n",
            "Epoch 97/100\n",
            "124/124 [==============================] - 5s 37ms/step - loss: 0.1142 - categorical_accuracy: 0.9244 - val_loss: 0.1474 - val_categorical_accuracy: 0.9298 - lr: 7.2689e-06\n",
            "Epoch 98/100\n",
            "124/124 [==============================] - 7s 54ms/step - loss: 0.1168 - categorical_accuracy: 0.9317 - val_loss: 0.1475 - val_categorical_accuracy: 0.9298 - lr: 6.9054e-06\n",
            "Epoch 99/100\n",
            "124/124 [==============================] - 5s 37ms/step - loss: 0.1116 - categorical_accuracy: 0.9320 - val_loss: 0.1474 - val_categorical_accuracy: 0.9298 - lr: 6.5601e-06\n",
            "Epoch 100/100\n",
            "124/124 [==============================] - 5s 43ms/step - loss: 0.1156 - categorical_accuracy: 0.9299 - val_loss: 0.1474 - val_categorical_accuracy: 0.9298 - lr: 6.2321e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, acc = model.evaluate(test_data, batch_size=1)\n",
        "print(f\"Test loss: {loss:.4f}, Test accuracy: {acc:.2%}\")"
      ],
      "metadata": {
        "id": "0V6OCj8JSMmj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0398432-024e-45c4-cbb9-9595a2e0ab3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1275/1275 [==============================] - 3s 2ms/step - loss: 0.1370 - categorical_accuracy: 0.9318\n",
            "Test loss: 0.1370, Test accuracy: 93.18%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.export_model(\"model.task\")"
      ],
      "metadata": {
        "id": "kmcb1tQxcI28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "deb3a769-e0fc-4940-fb72-5bb130fbf7d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://storage.googleapis.com/mediapipe-assets/gesture_embedder.tflite to /tmp/model_maker/gesture_recognizer/gesture_embedder.tflite\n",
            "Using existing files at /tmp/model_maker/gesture_recognizer/palm_detection_full.tflite\n",
            "Using existing files at /tmp/model_maker/gesture_recognizer/hand_landmark_full.tflite\n",
            "Downloading https://storage.googleapis.com/mediapipe-assets/canned_gesture_classifier.tflite to /tmp/model_maker/gesture_recognizer/canned_gesture_classifier.tflite\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_root = pathlib.Path(\"/content/processed_data/test\")\n",
        "testfiles = list(dataset_root.glob(\"**/*.jpg\"))  # Semua file JPG dalam dataset"
      ],
      "metadata": {
        "id": "sEQ3X2BygS9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = np.random.choice(testfiles)"
      ],
      "metadata": {
        "id": "wWF1geOlg1UW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(testfiles), testfiles[:10])  # Periksa jumlah dan contoh file gambar"
      ],
      "metadata": {
        "id": "vbAHQLBIhZDo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c913140-7456-4582-a0ae-75e58709fb4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1404 [PosixPath('/content/processed_data/test/D/IMG_20241117_195148.jpg'), PosixPath('/content/processed_data/test/D/IMG_20241025_192748.jpg'), PosixPath('/content/processed_data/test/D/IMG_20241117_195306.jpg'), PosixPath('/content/processed_data/test/D/IMG_20241117_195204.jpg'), PosixPath('/content/processed_data/test/D/IMG_20241025_093330.jpg'), PosixPath('/content/processed_data/test/D/IMG_20220224_172048.jpg'), PosixPath('/content/processed_data/test/D/Screenshot_20241024_222239_aug.jpg'), PosixPath('/content/processed_data/test/D/IMG_20220221_104349_aug.jpg'), PosixPath('/content/processed_data/test/D/IMG_20241025_093327.jpg'), PosixPath('/content/processed_data/test/D/IMG_20241117_195142.jpg')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mediapipe as mp\n",
        "from mediapipe.tasks.python.vision.gesture_recognizer import GestureRecognizer\n",
        "\n",
        "base_options = mp.tasks.BaseOptions(\n",
        "    model_asset_path=hparams.export_dir + \"/model.task\"\n",
        ")\n",
        "options = mp.tasks.vision.GestureRecognizerOptions(\n",
        "    base_options=base_options, running_mode=mp.tasks.vision.RunningMode.IMAGE\n",
        ")\n",
        "\n",
        "with GestureRecognizer.create_from_options(options) as recognizer:\n",
        "    mp_image = mp.Image.create_from_file(str(filename))\n",
        "    result = recognizer.recognize(mp_image)\n"
      ],
      "metadata": {
        "id": "B_D6wKF6c6_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_samples = np.random.choice(np.asarray(testfiles), 10)\n",
        "\n",
        "with GestureRecognizer.create_from_options(options) as recognizer:\n",
        "    fig, axarr = utils.plot_recognizer_predictions(test_samples, recognizer, 5)\n",
        "fig.savefig(\"example-output.jpg\", dpi=150, bbox_inches=\"tight\")"
      ],
      "metadata": {
        "id": "7Pyn-8jmg6RU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm"
      ],
      "metadata": {
        "id": "AwvZHSufhBPF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "678615d4-6742-4b12-8c64-c1d0d021bb02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm  # Tambahkan ini jika belum ada\n",
        "\n",
        "test_results = []\n",
        "with mp.tasks.vision.GestureRecognizer.create_from_options(options) as recognizer:\n",
        "    for filename in tqdm(testfiles, desc=\"Processing test files\"):\n",
        "        mp_image = mp.Image.create_from_file(str(filename))\n",
        "        result = recognizer.recognize(mp_image)\n",
        "        if len(result.gestures) > 0:\n",
        "            pred = result.gestures[0][0].category_name or \"n/a\"\n",
        "        else:\n",
        "            pred = \"empty\"\n",
        "        test_results.append((filename, filename.parent.name, pred))\n",
        "\n",
        "# Convert to DataFrame\n",
        "results_df = pd.DataFrame(test_results, columns=[\"filename\", \"label\", \"pred\"])\n"
      ],
      "metadata": {
        "id": "nSaBkNo1c_p3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01c32ffb-94c0-4918-e3b0-eeea0a1dd5db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test files: 100%|██████████| 1404/1404 [01:30<00:00, 15.52it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import numpy as np\n",
        "import sklearn\n",
        "\n",
        "# Menentukan urutan kelas\n",
        "classes = sorted(test_data.label_names + [\"n/a\", \"empty\"])\n",
        "\n",
        "# Menghitung confusion matrix tanpa normalisasi\n",
        "cm = sklearn.metrics.confusion_matrix(\n",
        "    results_df[\"label\"], results_df[\"pred\"], labels=classes\n",
        ")\n",
        "\n",
        "# Membuat objek ConfusionMatrixDisplay\n",
        "disp = sklearn.metrics.ConfusionMatrixDisplay(cm, display_labels=classes)\n",
        "\n",
        "# Membuat figure dan axis\n",
        "fig, ax = plt.subplots()  # Menyesuaikan ukuran untuk visualisasi lebih baik\n",
        "\n",
        "# Plot confusion matrix tanpa grid\n",
        "disp.plot(include_values=False, cmap=\"Blues\", ax=ax)\n",
        "ax.grid(False)\n",
        "ax.set_facecolor(\"white\")\n",
        "\n",
        "# Menambahkan nilai pada sel, hanya jika tidak nol, dengan warna abu-abu\n",
        "for i in range(cm.shape[0]):\n",
        "    for j in range(cm.shape[1]):\n",
        "        if cm[i, j] != 0:  # Menampilkan nilai hanya jika bukan nol\n",
        "            ax.text(\n",
        "                j, i, f\"{cm[i, j]}\",\n",
        "                ha=\"center\", va=\"center\", color=\"gold\", fontsize=8\n",
        "            )\n",
        "\n",
        "# Menyimpan plot ke file dan menampilkannya\n",
        "plt.savefig(\"confusion_matrix_filtered_gray_text.png\", dpi=150, bbox_inches=\"tight\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ILAKMIpRbzNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
        "\n",
        "# Menambahkan kolom 'result' untuk mengevaluasi apakah prediksi benar atau salah\n",
        "results_df[\"result\"] = results_df[\"pred\"] == results_df[\"label\"]\n",
        "\n",
        "# Menghitung precision, recall, dan f1-score untuk masing-masing kelas\n",
        "report = classification_report(\n",
        "    results_df[\"label\"],\n",
        "    results_df[\"pred\"],\n",
        "    labels=results_df[\"label\"].unique(),  # Memastikan semua label muncul di laporan\n",
        "    zero_division=0\n",
        ")\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n"
      ],
      "metadata": {
        "id": "Zt9Sa1hhOsTj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2539dbdd-7f3b-4b34-812a-ff2fea39b00a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           D       0.89      0.81      0.85        48\n",
            "           U       0.90      0.67      0.77        55\n",
            "           S       0.98      0.85      0.91        60\n",
            "           B       0.95      0.89      0.92        64\n",
            "           F       0.96      0.83      0.89        66\n",
            "           T       0.77      0.81      0.79        57\n",
            "           L       1.00      0.90      0.95        58\n",
            "           H       1.00      0.84      0.91        63\n",
            "           O       0.91      0.77      0.84        53\n",
            "           Y       1.00      0.95      0.97        60\n",
            "           M       1.00      0.83      0.91        60\n",
            "           E       0.98      0.89      0.93        64\n",
            "           C       1.00      0.75      0.86        48\n",
            "           X       0.98      0.74      0.84        68\n",
            "           A       1.00      0.97      0.99        69\n",
            "           V       0.96      0.80      0.88        61\n",
            "           I       0.95      0.91      0.93        64\n",
            "           G       1.00      0.75      0.85        63\n",
            "           R       0.89      0.77      0.82        61\n",
            "           W       1.00      0.88      0.94        59\n",
            "           N       0.98      0.93      0.96        60\n",
            "           Q       0.96      0.70      0.81        37\n",
            "           K       0.96      0.84      0.89        61\n",
            "           P       1.00      0.78      0.88        45\n",
            "\n",
            "   micro avg       0.96      0.83      0.89      1404\n",
            "   macro avg       0.96      0.83      0.89      1404\n",
            "weighted avg       0.96      0.83      0.89      1404\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
        "import pandas as pd\n",
        "\n",
        "# Menambahkan kolom 'result' untuk mengevaluasi apakah prediksi benar atau salah\n",
        "results_df[\"result\"] = results_df[\"pred\"] == results_df[\"label\"]\n",
        "\n",
        "# Menentukan urutan kelas dari A-Y\n",
        "classes = sorted(results_df[\"label\"].unique())  # Mengurutkan kelas dari A hingga Y\n",
        "\n",
        "# Menghitung precision, recall, dan f1-score untuk masing-masing kelas\n",
        "report = classification_report(\n",
        "    results_df[\"label\"],\n",
        "    results_df[\"pred\"],\n",
        "    labels=classes,  # Menambahkan urutan kelas\n",
        "    zero_division=0\n",
        ")\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n",
        "\n",
        "# Menambahkan nilai rata-rata precision, recall, f1-score\n",
        "precision_avg = precision_score(results_df[\"label\"], results_df[\"pred\"], average='macro', zero_division=0)\n",
        "recall_avg = recall_score(results_df[\"label\"], results_df[\"pred\"], average='macro', zero_division=0)\n",
        "f1_avg = f1_score(results_df[\"label\"], results_df[\"pred\"], average='macro', zero_division=0)\n",
        "\n",
        "print(\"\\nAverage Metrics:\")\n",
        "print(f\"Precision (average): {precision_avg:.4f}\")\n",
        "print(f\"Recall (average): {recall_avg:.4f}\")\n",
        "print(f\"F1 Score (average): {f1_avg:.4f}\")\n"
      ],
      "metadata": {
        "id": "oio_ZOrGPioA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengelompokkan hasil prediksi\n",
        "results_df[\"result\"] = np.where(\n",
        "    results_df.pred == results_df.label,\n",
        "    \"correct\",\n",
        "    np.where(results_df.pred.isin([\"empty\", \"n/a\"]), \"not found\", \"incorrect\")\n",
        ")\n",
        "\n",
        "# Mengatur urutan kategori untuk kolom 'result'\n",
        "results_df[\"result\"] = pd.Categorical(\n",
        "    results_df[\"result\"],\n",
        "    categories=[\"not found\", \"incorrect\", \"correct\"],\n",
        "    ordered=True\n",
        ")\n",
        "\n",
        "# Membuat urutan kategori dari A hingga Y\n",
        "label_order = sorted(results_df[\"label\"].unique())  # Menyortir label secara alfabetis\n",
        "\n",
        "# Mengubah kolom 'label' menjadi kategori dengan urutan yang sudah ditentukan\n",
        "results_df[\"label\"] = pd.Categorical(results_df[\"label\"], categories=label_order, ordered=True)\n",
        "\n",
        "# Atur gaya dan tema seaborn\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "sns.set_palette(\"pastel\")\n",
        "\n",
        "# Membuat histogram dengan seaborn\n",
        "plt.figure(figsize=(12, 8))\n",
        "ax = sns.histplot(\n",
        "    data=results_df,\n",
        "    x=\"label\",\n",
        "    hue=\"result\",\n",
        "    multiple=\"stack\",\n",
        "    stat=\"count\",\n",
        "    palette={\"correct\": \"mediumseagreen\", \"incorrect\": \"coral\", \"not found\": \"gray\"},  # Urutan warna sesuai kategori\n",
        "    legend=True\n",
        ")\n",
        "\n",
        "# Menambahkan judul dan label\n",
        "plt.title(\"Prediction Results by Label\", fontsize=16)\n",
        "plt.xlabel(\"Labels\", fontsize=14)\n",
        "plt.ylabel(\"Count\", fontsize=14)\n",
        "\n",
        "# Menampilkan legenda secara eksplisit\n",
        "plt.legend(\n",
        "    title=\"Result\",\n",
        "    title_fontsize=14,\n",
        "    fontsize=12,\n",
        "    loc=\"upper right\",\n",
        "    labels=[\"Correct\", \"Incorrect\", \"Not Found\"]  # Disesuaikan dengan urutan kategori\n",
        ")\n",
        "\n",
        "# Menyimpan grafik\n",
        "plt.savefig(\"prediction_results_with_ordered_labels.png\", bbox_inches=\"tight\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tbyrKa52mzuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df.query(\"result == 'not found'\").groupby(\n",
        "    \"label\"\n",
        ").pred.value_counts().sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "JMKD_8ubopRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = train_data.gen_tf_dataset(batch_size=train_data.size)\n",
        "xy = train_ds.take(1).get_single_element()\n",
        "\n",
        "embeddings, classes_onehot = xy[0].numpy(), xy[1].numpy()  # type: ignore\n",
        "class_indices = np.argmax(classes_onehot, axis=1)\n",
        "\n",
        "print(embeddings.shape, class_indices.shape)\n",
        "# -> (1861, 128) (1861,)"
      ],
      "metadata": {
        "id": "8FgDD3moo0FU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.manifold\n",
        "\n",
        "tsne = sklearn.manifold.TSNE()\n",
        "emb = tsne.fit_transform(embeddings)\n"
      ],
      "metadata": {
        "id": "G8HYUd1VQc1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "embdf = pd.DataFrame(emb, columns=[\"X1\", \"X2\"]).assign(label=class_indices)\n",
        "sns.scatterplot(\n",
        "    data=embdf, x=\"X1\", y=\"X2\", hue=\"label\", palette=\"Spectral\", legend=False\n",
        ")\n",
        "for i, c in enumerate(train_data.label_names):\n",
        "    if np.all(class_indices != i):\n",
        "        continue\n",
        "    center = emb[class_indices == i].mean(axis=0)\n",
        "    plt.annotate(c, center, center - 6)\n",
        "    plt.savefig(\"result.png\")\n"
      ],
      "metadata": {
        "id": "x_G9ankgQtiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "results_df[\"result\"] = np.where(\n",
        "    results_df.pred == results_df.label,\n",
        "    \"correct\",\n",
        "    np.where(results_df.pred.isin([\"n/a\", \"empty\"]), \"not found\", \"incorrect\"),\n",
        ")\n",
        "print(results_df.result.value_counts(normalize=True))\n",
        "sns.histplot(\n",
        "    data=results_df, x=\"label\", hue=\"result\", multiple=\"stack\", stat=\"count\"\n",
        ")"
      ],
      "metadata": {
        "id": "nNDm-tkhShMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "embdf = pd.DataFrame(emb, columns=[\"X1\", \"X2\"]).assign(label=class_indices)\n",
        "sns.scatterplot(\n",
        "    data=embdf, x=\"X1\", y=\"X2\", hue=\"label\", palette=\"Spectral\", legend=False\n",
        ")\n",
        "for i, c in enumerate(train_data.label_names):\n",
        "    if np.all(class_indices != i):\n",
        "        continue\n",
        "    center = emb[class_indices == i].mean(axis=0)\n",
        "    plt.annotate(c, center, center - 6)\n",
        "    plt.savefig(\"result.png\")\n"
      ],
      "metadata": {
        "id": "yEBdk--nS46v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}